{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAOf3bUiXi4Y",
        "outputId": "10600939-0e00-409d-e55a-4ed6cda7b206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip -q install pymupdf pdfminer.six pytesseract pillow\n",
        "!apt -q install -y tesseract-ocr >/dev/null\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab:\n",
        "!pip -q install pymupdf==1.24.9\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwgiZWLKUhrB",
        "outputId": "185aae45-8fc6-4f4d-f7c5-120396de23f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === AUTO LIB : Scan dossier -> PDF->JSONL->FIXED + index ===\n",
        "!pip -q install pymupdf==1.24.9\n",
        "import os, re, json, hashlib, unicodedata, time, fitz\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "# ---------- Normalisation forte ----------\n",
        "ZW = {\"\\u200B\",\"\\u200C\",\"\\u200D\",\"\\u2060\",\"\\uFEFF\"}\n",
        "LIG={\"ﬀ\":\"ff\",\"ﬁ\":\"fi\",\"ﬂ\":\"fl\",\"ﬃ\":\"ffi\",\"ﬄ\":\"ffl\",\"ﬅ\":\"ft\",\"ﬆ\":\"st\"}\n",
        "QUO={\"“\":'\"',\"”\":'\"',\"„\":'\"',\"«\":'\"',\"»\":'\"',\"‘\":\"'\", \"’\":\"'\", \"‚\":\"'\"}\n",
        "DASH={\"‒\":\"-\",\"‐\":\"-\",\"–\":\"-\",\"—\":\"-\",\"−\":\"-\",\"\\u2010\":\"-\",\"\\u2011\":\"-\",\"\\u2012\":\"-\",\"\\u2212\":\"-\"}\n",
        "def norm_strong(s:str)->str:\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    for z in ZW: s = s.replace(z,\"\")\n",
        "    s = s.replace(\"\\xa0\",\" \")\n",
        "    for m in (LIG,QUO,DASH):\n",
        "        for k,v in m.items(): s = s.replace(k,v)\n",
        "    s = s.replace(\"…\",\"...\")\n",
        "    s = re.sub(r\"\\s+([,.;:!?%)\\]}])\", r\"\\1\", s)\n",
        "    s = re.sub(r\"([([{])\\s+\", r\"\\1\", s)\n",
        "    s = re.sub(r\"\\s+%\",\"%\", s)\n",
        "    s = re.sub(r\"[ \\t]+\",\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def _sha(s:str)->str: return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "# ---------- Extraction \"lossless\" ----------\n",
        "@dataclass\n",
        "class Word:\n",
        "    text: str; x0: float; y0: float; x1: float; y1: float; block: int; line: int; wno: int\n",
        "\n",
        "def get_words_lossless(page):\n",
        "    out, seen = [], set()\n",
        "    for (x0,y0,x1,y1,txt,b,l,w) in page.get_text(\"words\"):\n",
        "        if not txt: continue\n",
        "        key=(round(x0,2),round(y0,2),round(x1,2),round(y1,2),txt,b,l,w)\n",
        "        if key in seen: continue\n",
        "        seen.add(key); out.append(Word(txt,x0,y0,x1,y1,b,l,w))\n",
        "    out.sort(key=lambda w:(w.block,w.line,w.wno,w.x0,w.y0))\n",
        "    return out\n",
        "\n",
        "PUNCT_STICKY={\",\",\".\",\";\",\";\",\":\",\"!\",\"?\",\")\",\"]\",\"}\",\"%\"}\n",
        "PUNCT_OPEN=set([\"(\",\"[\",\"{\",\"«\",\"“\",\"‘\"])\n",
        "def join_words_into_text(words: List[Word]) -> str:\n",
        "    res=[]; prev=None\n",
        "    for w in words:\n",
        "        t=w.text\n",
        "        if not res: res.append(t)\n",
        "        else:\n",
        "            if t in PUNCT_STICKY: res.append(t)\n",
        "            elif prev and prev[-1] in PUNCT_OPEN: res.append(t)\n",
        "            else: res.append(\" \"+t)\n",
        "        prev=t\n",
        "    return norm_strong(\"\".join(res))\n",
        "\n",
        "def page_to_json(page_idx, page):\n",
        "    words = get_words_lossless(page)\n",
        "    text  = join_words_into_text(words)\n",
        "    return {\n",
        "        \"page_index\": page_idx,\n",
        "        \"size\": {\"width\": page.rect.width, \"height\": page.rect.height},\n",
        "        \"words\": [asdict(w) for w in words],\n",
        "        \"text\": text,\n",
        "        \"stats\": {\"n_words\": len(words), \"n_chars\": len(text), \"sha256\": _sha(text)}\n",
        "    }\n",
        "\n",
        "# ---------- Conversion + manifest ----------\n",
        "def pdf_to_jsonl(pdf_path:str, out_jsonl:str, manifest_path:str):\n",
        "    os.makedirs(os.path.dirname(out_jsonl) or \".\", exist_ok=True)\n",
        "    doc = fitz.open(pdf_path)\n",
        "    report=[]; ours_tot=0; ref_tot=0; ok=0\n",
        "    with open(out_jsonl,\"w\",encoding=\"utf-8\") as f:\n",
        "        for i in range(len(doc)):\n",
        "            page=doc[i]\n",
        "            pj=page_to_json(i,page)\n",
        "            ours = pj[\"text\"]\n",
        "            ref  = norm_strong(page.get_text(\"text\").replace(\"\\r\",\"\\n\").replace(\"\\n\",\" \"))\n",
        "            same = _sha(ours)==_sha(ref)\n",
        "            if same: ok+=1\n",
        "            ours_tot += len(ours); ref_tot += len(ref)\n",
        "            f.write(json.dumps(pj, ensure_ascii=False)+\"\\n\")\n",
        "            pref=len(os.path.commonprefix([ours,ref])); cov=round(pref/max(1,len(ref)),6)\n",
        "            report.append({\"page_index\":i,\"same_len\":len(ours)==len(ref),\"same_sha\":same,\n",
        "                           \"coverage_char_ratio\":cov,\"n_chars_ours\":len(ours),\"n_chars_ref\":len(ref)})\n",
        "    doc.close()\n",
        "    manifest={\"pdf_path\":pdf_path,\"n_pages\":len(report),\"pages_ok_same_sha\":ok,\n",
        "              \"global_chars_ours\":ours_tot,\"global_chars_ref\":ref_tot,\n",
        "              \"global_same_charcount\":ours_tot==ref_tot,\"pages_report\":report}\n",
        "    with open(manifest_path,\"w\",encoding=\"utf-8\") as mf:\n",
        "        json.dump(manifest, mf, ensure_ascii=False, indent=2)\n",
        "\n",
        "# ---------- Fix pages douteuses (fallback contrôlé) ----------\n",
        "def make_authoritative_jsonl(pdf_path:str, in_jsonl:str, out_jsonl:str, cov_threshold=0.98):\n",
        "    doc=fitz.open(pdf_path)\n",
        "    with open(in_jsonl,encoding=\"utf-8\") as fi, open(out_jsonl,\"w\",encoding=\"utf-8\") as fo:\n",
        "        for line in fi:\n",
        "            o=json.loads(line)\n",
        "            p=o[\"page_index\"]\n",
        "            ours=norm_strong(o[\"text\"])\n",
        "            ref =norm_strong(doc[p].get_text(\"text\").replace(\"\\r\",\"\\n\").replace(\"\\n\",\" \"))\n",
        "            if _sha(ours)!=_sha(ref):\n",
        "                pref=len(os.path.commonprefix([ours,ref])); cov=pref/max(1,len(ref))\n",
        "                if cov<cov_threshold:\n",
        "                    o[\"text\"]=ref\n",
        "                    o.setdefault(\"debug\",{})[\"fallback_applied\"]=True\n",
        "                    o[\"debug\"][\"coverage_before\"]=round(cov,6)\n",
        "            fo.write(json.dumps(o, ensure_ascii=False)+\"\\n\")\n",
        "    doc.close()\n",
        "\n",
        "# ---------- Utils scan ----------\n",
        "def list_pdfs(input_dir:str)->List[str]:\n",
        "    pdfs=[]\n",
        "    for root,_,files in os.walk(input_dir):\n",
        "        for fn in files:\n",
        "            if fn.lower().endswith(\".pdf\"):\n",
        "                pdfs.append(os.path.join(root, fn))\n",
        "    return sorted(pdfs)\n",
        "\n",
        "def relpath_without_ext(path:str, base_dir:str)->str:\n",
        "    rel = os.path.relpath(path, base_dir)\n",
        "    return os.path.splitext(rel)[0]  # ex: \"sub/a/b/c\"\n",
        "\n",
        "# ---------- Orchestrateur full-auto ----------\n",
        "def process_pdf(pdf_path:str, input_dir:str, output_dir:str, cov_threshold=0.98)->Dict:\n",
        "    rel = relpath_without_ext(pdf_path, input_dir)\n",
        "    out_dir = os.path.join(output_dir, os.path.dirname(rel))\n",
        "    base    = os.path.basename(rel)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    out_jsonl   = os.path.join(out_dir, f\"{base}.jsonl\")\n",
        "    out_manifest= os.path.join(out_dir, f\"{base}_manifest.json\")\n",
        "    out_fixed   = os.path.join(out_dir, f\"{base}_fixed.jsonl\")\n",
        "\n",
        "    # skip si déjà à jour (fixed plus récent que le pdf)\n",
        "    if os.path.exists(out_fixed) and os.path.getmtime(out_fixed) >= os.path.getmtime(pdf_path):\n",
        "        # lire manifest pour stats si dispo, sinon fabriquer minimal\n",
        "        manifest = {}\n",
        "        if os.path.exists(out_manifest):\n",
        "            with open(out_manifest, encoding=\"utf-8\") as mf: manifest = json.load(mf)\n",
        "        return {\"pdf\": pdf_path, \"status\":\"skipped(up-to-date)\", \"outputs\":{\n",
        "            \"jsonl\": out_jsonl, \"fixed_jsonl\": out_fixed, \"manifest\": out_manifest\n",
        "        }, \"manifest\": manifest}\n",
        "\n",
        "    # étape 1: pdf -> jsonl + manifest\n",
        "    pdf_to_jsonl(pdf_path, out_jsonl, out_manifest)\n",
        "    # étape 2: autoritative fixed\n",
        "    make_authoritative_jsonl(pdf_path, out_jsonl, out_fixed, cov_threshold=cov_threshold)\n",
        "\n",
        "    # résumer pages corrigées\n",
        "    fixed_pages=[]; total_pages=0\n",
        "    with open(out_fixed, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            o=json.loads(line); total_pages+=1\n",
        "            if o.get(\"debug\",{}).get(\"fallback_applied\"): fixed_pages.append(o[\"page_index\"])\n",
        "\n",
        "    return {\"pdf\": pdf_path, \"status\":\"processed\", \"pages_total\": total_pages,\n",
        "            \"pages_fixed\": fixed_pages, \"outputs\":{\n",
        "                \"jsonl\": out_jsonl, \"fixed_jsonl\": out_fixed, \"manifest\": out_manifest\n",
        "            }}\n",
        "\n",
        "def process_all(input_dir:str=\"/content/in\", output_dir:str=\"/content/out\", cov_threshold=0.98)->Dict:\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    pdfs = list_pdfs(input_dir)\n",
        "    summary = {\"input_dir\": input_dir, \"output_dir\": output_dir, \"count\": len(pdfs), \"files\": []}\n",
        "    for i,p in enumerate(pdfs,1):\n",
        "        print(f\"[{i}/{len(pdfs)}] {p}\")\n",
        "        try:\n",
        "            res = process_pdf(p, input_dir, output_dir, cov_threshold)\n",
        "        except Exception as e:\n",
        "            res = {\"pdf\": p, \"status\": f\"error: {e}\"}\n",
        "        summary[\"files\"].append(res)\n",
        "\n",
        "    # index.json pour tracer tout\n",
        "    index_path = os.path.join(output_dir, \"index.json\")\n",
        "    with open(index_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "    print(\"OK index:\", index_path)\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "-JsoU7_Ims89"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# crée les dossiers si besoin\n",
        "os.makedirs(\"/content/\", exist_ok=True)\n",
        "os.makedirs(\"/content/\", exist_ok=True)\n",
        "\n",
        "# 1 commande = scan + conversion + fix + index\n",
        "summary = process_all(input_dir=\"/content/\", output_dir=\"/content/\", cov_threshold=0.98)\n",
        "\n",
        "# aperçu rapide\n",
        "print(\"Trouvés:\", summary[\"count\"], \"PDFs\")\n",
        "for f in summary[\"files\"][:10]:\n",
        "    print(f[\"status\"], \"->\", f.get(\"outputs\",{}).get(\"fixed_jsonl\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nwp8Jv1fmwso",
        "outputId": "0164f2a5-9a2b-4a5a-f09f-fd5ba555bc2f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/4] /content/0018_LeveragingSecurity_DK_20160111.pdf\n",
            "[2/4] /content/0019_AutomatingProt_DK_20160111.pdf\n",
            "[3/4] /content/511399-UEN_CSDG_670_2p2.pdf\n",
            "[4/4] /content/SIP5_Security_V10.00_Manual_C081-G_en.pdf\n",
            "OK index: /content/index.json\n",
            "Trouvés: 4 PDFs\n",
            "processed -> /content/0018_LeveragingSecurity_DK_20160111_fixed.jsonl\n",
            "processed -> /content/0019_AutomatingProt_DK_20160111_fixed.jsonl\n",
            "processed -> /content/511399-UEN_CSDG_670_2p2_fixed.jsonl\n",
            "processed -> /content/SIP5_Security_V10.00_Manual_C081-G_en_fixed.jsonl\n"
          ]
        }
      ]
    }
  ]
}