{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIyELm5D9cVSypdbUj5Zet",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariemBjbr/portfolio-mariem/blob/main/extractPDF_To_JSOn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAOf3bUiXi4Y",
        "outputId": "10600939-0e00-409d-e55a-4ed6cda7b206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip -q install pymupdf pdfminer.six pytesseract pillow\n",
        "!apt -q install -y tesseract-ocr >/dev/null\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab:\n",
        "!pip -q install pymupdf==1.24.9\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwgiZWLKUhrB",
        "outputId": "185aae45-8fc6-4f4d-f7c5-120396de23f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === AUTO LIB : Scan dossier -> PDF->JSONL->FIXED + index ===\n",
        "!pip -q install pymupdf==1.24.9\n",
        "import os, re, json, hashlib, unicodedata, time, fitz\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "# ---------- Normalisation forte ----------\n",
        "ZW = {\"\\u200B\",\"\\u200C\",\"\\u200D\",\"\\u2060\",\"\\uFEFF\"}\n",
        "LIG={\"ﬀ\":\"ff\",\"ﬁ\":\"fi\",\"ﬂ\":\"fl\",\"ﬃ\":\"ffi\",\"ﬄ\":\"ffl\",\"ﬅ\":\"ft\",\"ﬆ\":\"st\"}\n",
        "QUO={\"“\":'\"',\"”\":'\"',\"„\":'\"',\"«\":'\"',\"»\":'\"',\"‘\":\"'\", \"’\":\"'\", \"‚\":\"'\"}\n",
        "DASH={\"‒\":\"-\",\"‐\":\"-\",\"–\":\"-\",\"—\":\"-\",\"−\":\"-\",\"\\u2010\":\"-\",\"\\u2011\":\"-\",\"\\u2012\":\"-\",\"\\u2212\":\"-\"}\n",
        "def norm_strong(s:str)->str:\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    for z in ZW: s = s.replace(z,\"\")\n",
        "    s = s.replace(\"\\xa0\",\" \")\n",
        "    for m in (LIG,QUO,DASH):\n",
        "        for k,v in m.items(): s = s.replace(k,v)\n",
        "    s = s.replace(\"…\",\"...\")\n",
        "    s = re.sub(r\"\\s+([,.;:!?%)\\]}])\", r\"\\1\", s)\n",
        "    s = re.sub(r\"([([{])\\s+\", r\"\\1\", s)\n",
        "    s = re.sub(r\"\\s+%\",\"%\", s)\n",
        "    s = re.sub(r\"[ \\t]+\",\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def _sha(s:str)->str: return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "# ---------- Extraction \"lossless\" ----------\n",
        "@dataclass\n",
        "class Word:\n",
        "    text: str; x0: float; y0: float; x1: float; y1: float; block: int; line: int; wno: int\n",
        "\n",
        "def get_words_lossless(page):\n",
        "    out, seen = [], set()\n",
        "    for (x0,y0,x1,y1,txt,b,l,w) in page.get_text(\"words\"):\n",
        "        if not txt: continue\n",
        "        key=(round(x0,2),round(y0,2),round(x1,2),round(y1,2),txt,b,l,w)\n",
        "        if key in seen: continue\n",
        "        seen.add(key); out.append(Word(txt,x0,y0,x1,y1,b,l,w))\n",
        "    out.sort(key=lambda w:(w.block,w.line,w.wno,w.x0,w.y0))\n",
        "    return out\n",
        "\n",
        "PUNCT_STICKY={\",\",\".\",\";\",\";\",\":\",\"!\",\"?\",\")\",\"]\",\"}\",\"%\"}\n",
        "PUNCT_OPEN=set([\"(\",\"[\",\"{\",\"«\",\"“\",\"‘\"])\n",
        "def join_words_into_text(words: List[Word]) -> str:\n",
        "    res=[]; prev=None\n",
        "    for w in words:\n",
        "        t=w.text\n",
        "        if not res: res.append(t)\n",
        "        else:\n",
        "            if t in PUNCT_STICKY: res.append(t)\n",
        "            elif prev and prev[-1] in PUNCT_OPEN: res.append(t)\n",
        "            else: res.append(\" \"+t)\n",
        "        prev=t\n",
        "    return norm_strong(\"\".join(res))\n",
        "\n",
        "def page_to_json(page_idx, page):\n",
        "    words = get_words_lossless(page)\n",
        "    text  = join_words_into_text(words)\n",
        "    return {\n",
        "        \"page_index\": page_idx,\n",
        "        \"size\": {\"width\": page.rect.width, \"height\": page.rect.height},\n",
        "        \"words\": [asdict(w) for w in words],\n",
        "        \"text\": text,\n",
        "        \"stats\": {\"n_words\": len(words), \"n_chars\": len(text), \"sha256\": _sha(text)}\n",
        "    }\n",
        "\n",
        "# ---------- Conversion + manifest ----------\n",
        "def pdf_to_jsonl(pdf_path:str, out_jsonl:str, manifest_path:str):\n",
        "    os.makedirs(os.path.dirname(out_jsonl) or \".\", exist_ok=True)\n",
        "    doc = fitz.open(pdf_path)\n",
        "    report=[]; ours_tot=0; ref_tot=0; ok=0\n",
        "    with open(out_jsonl,\"w\",encoding=\"utf-8\") as f:\n",
        "        for i in range(len(doc)):\n",
        "            page=doc[i]\n",
        "            pj=page_to_json(i,page)\n",
        "            ours = pj[\"text\"]\n",
        "            ref  = norm_strong(page.get_text(\"text\").replace(\"\\r\",\"\\n\").replace(\"\\n\",\" \"))\n",
        "            same = _sha(ours)==_sha(ref)\n",
        "            if same: ok+=1\n",
        "            ours_tot += len(ours); ref_tot += len(ref)\n",
        "            f.write(json.dumps(pj, ensure_ascii=False)+\"\\n\")\n",
        "            pref=len(os.path.commonprefix([ours,ref])); cov=round(pref/max(1,len(ref)),6)\n",
        "            report.append({\"page_index\":i,\"same_len\":len(ours)==len(ref),\"same_sha\":same,\n",
        "                           \"coverage_char_ratio\":cov,\"n_chars_ours\":len(ours),\"n_chars_ref\":len(ref)})\n",
        "    doc.close()\n",
        "    manifest={\"pdf_path\":pdf_path,\"n_pages\":len(report),\"pages_ok_same_sha\":ok,\n",
        "              \"global_chars_ours\":ours_tot,\"global_chars_ref\":ref_tot,\n",
        "              \"global_same_charcount\":ours_tot==ref_tot,\"pages_report\":report}\n",
        "    with open(manifest_path,\"w\",encoding=\"utf-8\") as mf:\n",
        "        json.dump(manifest, mf, ensure_ascii=False, indent=2)\n",
        "\n",
        "# ---------- Fix pages douteuses (fallback contrôlé) ----------\n",
        "def make_authoritative_jsonl(pdf_path:str, in_jsonl:str, out_jsonl:str, cov_threshold=0.98):\n",
        "    doc=fitz.open(pdf_path)\n",
        "    with open(in_jsonl,encoding=\"utf-8\") as fi, open(out_jsonl,\"w\",encoding=\"utf-8\") as fo:\n",
        "        for line in fi:\n",
        "            o=json.loads(line)\n",
        "            p=o[\"page_index\"]\n",
        "            ours=norm_strong(o[\"text\"])\n",
        "            ref =norm_strong(doc[p].get_text(\"text\").replace(\"\\r\",\"\\n\").replace(\"\\n\",\" \"))\n",
        "            if _sha(ours)!=_sha(ref):\n",
        "                pref=len(os.path.commonprefix([ours,ref])); cov=pref/max(1,len(ref))\n",
        "                if cov<cov_threshold:\n",
        "                    o[\"text\"]=ref\n",
        "                    o.setdefault(\"debug\",{})[\"fallback_applied\"]=True\n",
        "                    o[\"debug\"][\"coverage_before\"]=round(cov,6)\n",
        "            fo.write(json.dumps(o, ensure_ascii=False)+\"\\n\")\n",
        "    doc.close()\n",
        "\n",
        "# ---------- Utils scan ----------\n",
        "def list_pdfs(input_dir:str)->List[str]:\n",
        "    pdfs=[]\n",
        "    for root,_,files in os.walk(input_dir):\n",
        "        for fn in files:\n",
        "            if fn.lower().endswith(\".pdf\"):\n",
        "                pdfs.append(os.path.join(root, fn))\n",
        "    return sorted(pdfs)\n",
        "\n",
        "def relpath_without_ext(path:str, base_dir:str)->str:\n",
        "    rel = os.path.relpath(path, base_dir)\n",
        "    return os.path.splitext(rel)[0]  # ex: \"sub/a/b/c\"\n",
        "\n",
        "# ---------- Orchestrateur full-auto ----------\n",
        "def process_pdf(pdf_path:str, input_dir:str, output_dir:str, cov_threshold=0.98)->Dict:\n",
        "    rel = relpath_without_ext(pdf_path, input_dir)\n",
        "    out_dir = os.path.join(output_dir, os.path.dirname(rel))\n",
        "    base    = os.path.basename(rel)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    out_jsonl   = os.path.join(out_dir, f\"{base}.jsonl\")\n",
        "    out_manifest= os.path.join(out_dir, f\"{base}_manifest.json\")\n",
        "    out_fixed   = os.path.join(out_dir, f\"{base}_fixed.jsonl\")\n",
        "\n",
        "    # skip si déjà à jour (fixed plus récent que le pdf)\n",
        "    if os.path.exists(out_fixed) and os.path.getmtime(out_fixed) >= os.path.getmtime(pdf_path):\n",
        "        # lire manifest pour stats si dispo, sinon fabriquer minimal\n",
        "        manifest = {}\n",
        "        if os.path.exists(out_manifest):\n",
        "            with open(out_manifest, encoding=\"utf-8\") as mf: manifest = json.load(mf)\n",
        "        return {\"pdf\": pdf_path, \"status\":\"skipped(up-to-date)\", \"outputs\":{\n",
        "            \"jsonl\": out_jsonl, \"fixed_jsonl\": out_fixed, \"manifest\": out_manifest\n",
        "        }, \"manifest\": manifest}\n",
        "\n",
        "    # étape 1: pdf -> jsonl + manifest\n",
        "    pdf_to_jsonl(pdf_path, out_jsonl, out_manifest)\n",
        "    # étape 2: autoritative fixed\n",
        "    make_authoritative_jsonl(pdf_path, out_jsonl, out_fixed, cov_threshold=cov_threshold)\n",
        "\n",
        "    # résumer pages corrigées\n",
        "    fixed_pages=[]; total_pages=0\n",
        "    with open(out_fixed, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            o=json.loads(line); total_pages+=1\n",
        "            if o.get(\"debug\",{}).get(\"fallback_applied\"): fixed_pages.append(o[\"page_index\"])\n",
        "\n",
        "    return {\"pdf\": pdf_path, \"status\":\"processed\", \"pages_total\": total_pages,\n",
        "            \"pages_fixed\": fixed_pages, \"outputs\":{\n",
        "                \"jsonl\": out_jsonl, \"fixed_jsonl\": out_fixed, \"manifest\": out_manifest\n",
        "            }}\n",
        "\n",
        "def process_all(input_dir:str=\"/content/in\", output_dir:str=\"/content/out\", cov_threshold=0.98)->Dict:\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    pdfs = list_pdfs(input_dir)\n",
        "    summary = {\"input_dir\": input_dir, \"output_dir\": output_dir, \"count\": len(pdfs), \"files\": []}\n",
        "    for i,p in enumerate(pdfs,1):\n",
        "        print(f\"[{i}/{len(pdfs)}] {p}\")\n",
        "        try:\n",
        "            res = process_pdf(p, input_dir, output_dir, cov_threshold)\n",
        "        except Exception as e:\n",
        "            res = {\"pdf\": p, \"status\": f\"error: {e}\"}\n",
        "        summary[\"files\"].append(res)\n",
        "\n",
        "    # index.json pour tracer tout\n",
        "    index_path = os.path.join(output_dir, \"index.json\")\n",
        "    with open(index_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "    print(\"OK index:\", index_path)\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "-JsoU7_Ims89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "os.makedirs(\"/content/\", exist_ok=True)\n",
        "os.makedirs(\"/content/\", exist_ok=True)\n",
        "\n",
        "summary = process_all(input_dir=\"/content/\", output_dir=\"/content/\", cov_threshold=0.98)\n",
        "\n",
        "\n",
        "print(\"Trouvés:\", summary[\"count\"], \"PDFs\")\n",
        "for f in summary[\"files\"][:10]:\n",
        "    print(f[\"status\"], \"->\", f.get(\"outputs\",{}).get(\"fixed_jsonl\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nwp8Jv1fmwso",
        "outputId": "0164f2a5-9a2b-4a5a-f09f-fd5ba555bc2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/4] /content/0018_LeveragingSecurity_DK_20160111.pdf\n",
            "[2/4] /content/0019_AutomatingProt_DK_20160111.pdf\n",
            "[3/4] /content/511399-UEN_CSDG_670_2p2.pdf\n",
            "[4/4] /content/SIP5_Security_V10.00_Manual_C081-G_en.pdf\n",
            "OK index: /content/index.json\n",
            "Trouvés: 4 PDFs\n",
            "processed -> /content/0018_LeveragingSecurity_DK_20160111_fixed.jsonl\n",
            "processed -> /content/0019_AutomatingProt_DK_20160111_fixed.jsonl\n",
            "processed -> /content/511399-UEN_CSDG_670_2p2_fixed.jsonl\n",
            "processed -> /content/SIP5_Security_V10.00_Manual_C081-G_en_fixed.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Étape 1 — Chunking sûr (depuis _fixed.jsonl)**\n",
        "\n",
        "saute les pages vides (images)\n",
        "\n",
        "coupe par paragraphes\n",
        "\n",
        "cible ~1000 tokens (approx), overlap 120\n",
        "\n",
        "garde la traçabilité des pages"
      ],
      "metadata": {
        "id": "I7r2F53dwTDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, re, os\n",
        "\n",
        "def rough_token_count(s:str)->int:\n",
        "    # approx simple ~ 0.75 * nb_mots\n",
        "    return int(0.75 * len(re.findall(r\"\\S+\", s)))\n",
        "\n",
        "def paragraph_iter(text:str):\n",
        "    paras = [p.strip() for p in re.split(r\"\\n{2,}\", text) if p.strip()]\n",
        "    return paras if paras else ([text.strip()] if text.strip() else [])\n",
        "\n",
        "def build_chunks_from_fixed_jsonl(fixed_jsonl:str, target_tokens=1000, overlap_tokens=120, min_tokens=200):\n",
        "    pages=[]\n",
        "    with open(fixed_jsonl, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            o=json.loads(line)\n",
        "            if not o[\"text\"].strip():   # ignore pages vides (images)\n",
        "                continue\n",
        "            pages.append((o[\"page_index\"], o[\"text\"]))\n",
        "\n",
        "    chunks, buf, buf_tok, buf_pages = [], [], 0, set()\n",
        "    for page_idx, text in pages:\n",
        "        for para in paragraph_iter(text):\n",
        "            t = rough_token_count(para)\n",
        "            if buf_tok and buf_tok + t > target_tokens:\n",
        "                chunks.append({\"text\":\"\\n\\n\".join(buf).strip(),\n",
        "                               \"approx_tokens\":buf_tok,\n",
        "                               \"pages\":sorted(buf_pages)})\n",
        "                # overlap\n",
        "                tail, tt = [], 0\n",
        "                for p in reversed(buf):\n",
        "                    pt = rough_token_count(p)\n",
        "                    if tt + pt > overlap_tokens: break\n",
        "                    tail.insert(0, p); tt += pt\n",
        "                buf, buf_tok = tail, sum(rough_token_count(x) for x in tail)\n",
        "                buf_pages = set()\n",
        "            buf.append(para); buf_tok += t; buf_pages.add(page_idx)\n",
        "    if buf:\n",
        "        chunks.append({\"text\":\"\\n\\n\".join(buf).strip(),\n",
        "                       \"approx_tokens\":buf_tok,\n",
        "                       \"pages\":sorted(buf_pages)})\n",
        "\n",
        "    # filtre micro-chunks\n",
        "    chunks = [c for c in chunks if c[\"approx_tokens\"] >= min_tokens or len(chunks)==1]\n",
        "    return chunks\n",
        "\n",
        "# === Exécuter (adapter base) ===\n",
        "base = \"/content/0018_LeveragingSecurity_DK_20160111\"\n",
        "fixed_jsonl = f\"{base}_fixed.jsonl\"\n",
        "out_chunks = f\"{base}_chunks.jsonl\"\n",
        "\n",
        "chunks = build_chunks_from_fixed_jsonl(fixed_jsonl, target_tokens=1000, overlap_tokens=120, min_tokens=200)\n",
        "with open(out_chunks, \"w\", encoding=\"utf-8\") as f:\n",
        "    for c in chunks:\n",
        "        f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"OK chunks:\", len(chunks), \"->\", out_chunks)\n",
        "print(\"Exemple pages du 1er chunk:\", chunks[0][\"pages\"][:10], \"tokens~\", chunks[0][\"approx_tokens\"])\n"
      ],
      "metadata": {
        "id": "5kKu4JcGwZqI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b30ae3a8-d064-4380-e1a8-01f6e487ba62"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK chunks: 4 -> /content/0018_LeveragingSecurity_DK_20160111_chunks.jsonl\n",
            "Exemple pages du 1er chunk: [0, 1, 2, 3] tokens~ 949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vérif “rien perdu” (par page + global)**"
      ],
      "metadata": {
        "id": "H8iC8t21-f5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, fitz, re, unicodedata, hashlib\n",
        "\n",
        "def _sha(s): return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "ZW = {\"\\u200B\",\"\\u200C\",\"\\u200D\",\"\\u2060\",\"\\uFEFF\"}\n",
        "LIG={\"ﬀ\":\"ff\",\"ﬁ\":\"fi\",\"ﬂ\":\"fl\",\"ﬃ\":\"ffi\",\"ﬄ\":\"ffl\",\"ﬅ\":\"ft\",\"ﬆ\":\"st\"}\n",
        "QUO={\"“\":'\"',\"”\":'\"',\"„\":'\"',\"«\":'\"',\"»\":'\"',\"‘\":\"'\", \"’\":\"'\", \"‚\":\"'\"}\n",
        "DASH={\"‒\":\"-\",\"‐\":\"-\",\"–\":\"-\",\"—\":\"-\",\"−\":\"-\",\"\\u2010\":\"-\",\"\\u2011\":\"-\",\"\\u2012\":\"-\",\"\\u2212\":\"-\"}\n",
        "def norm_strong(s:str)->str:\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    for z in ZW: s = s.replace(z,\"\")\n",
        "    s = s.replace(\"\\xa0\",\" \")\n",
        "    for m in (LIG,QUO,DASH):\n",
        "        for k,v in m.items(): s = s.replace(k,v)\n",
        "    s = s.replace(\"…\",\"...\")\n",
        "    s = re.sub(r\"\\s+([,.;:!?%)\\]}])\", r\"\\1\", s)\n",
        "    s = re.sub(r\"([([{])\\s+\", r\"\\1\", s)\n",
        "    s = re.sub(r\"\\s+%\",\"%\", s)\n",
        "    s = re.sub(r\"[ \\t]+\",\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def verify_fixed_against_pdf(pdf_path:str, fixed_jsonl:str):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    ok, tot = 0, 0\n",
        "    diffs = []\n",
        "    ours_sum = []; ref_sum = []\n",
        "    with open(fixed_jsonl, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            o = json.loads(line); p = o[\"page_index\"]; tot += 1\n",
        "            ours = norm_strong(o[\"text\"])\n",
        "            ref  = norm_strong(doc[p].get_text(\"text\").replace(\"\\r\",\"\\n\").replace(\"\\n\",\" \"))\n",
        "            if _sha(ours)==_sha(ref):\n",
        "                ok += 1\n",
        "            else:\n",
        "                pref = len(os.path.commonprefix([ours,ref]))\n",
        "                cov  = round(pref/max(1,len(ref)),6)\n",
        "                diffs.append({\"page_index\":p,\"coverage_char_ratio\":cov,\n",
        "                              \"n_chars_ours\":len(ours),\"n_chars_ref\":len(ref)})\n",
        "            ours_sum.append(ours); ref_sum.append(ref)\n",
        "    doc.close()\n",
        "    global_ok = _sha(\"\".join(ours_sum)) == _sha(\"\".join(ref_sum))\n",
        "    return {\"pages_ok\": ok, \"pages_total\": tot, \"global_ok\": global_ok, \"bad_pages\": diffs}\n",
        "\n",
        "# EXEMPLE\n",
        "pdf = \"/content/0018_LeveragingSecurity_DK_20160111.pdf\"\n",
        "fixed = \"/content/0018_LeveragingSecurity_DK_20160111_fixed.jsonl\"\n",
        "res = verify_fixed_against_pdf(pdf, fixed)\n",
        "print(res)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGORifhE-iLa",
        "outputId": "4935385f-9e6b-4ba0-fa69-4ca56c47862a"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pages_ok': 14, 'pages_total': 14, 'global_ok': True, 'bad_pages': []}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Audit couverture des chunks (est-ce que les chunks couvrent tout le texte ?)**"
      ],
      "metadata": {
        "id": "apig2GjH-mnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, re\n",
        "\n",
        "def _paragraphs(text, min_len=80):\n",
        "    # Paragraphes = blocs séparés par >=1 ligne vide\n",
        "    paras = [p.strip() for p in re.split(r\"\\n{2,}\", text) if p.strip()]\n",
        "    return [p for p in paras if len(p) >= min_len]\n",
        "\n",
        "def _union_len(spans):\n",
        "    if not spans: return 0\n",
        "    spans = sorted(spans)\n",
        "    merged = []\n",
        "    cs, ce = spans[0]\n",
        "    for s,e in spans[1:]:\n",
        "        if s <= ce: ce = max(ce, e)\n",
        "        else: merged.append((cs, ce)); cs, ce = s, e\n",
        "    merged.append((cs, ce))\n",
        "    return sum(e-s for s,e in merged)\n",
        "\n",
        "def _load_pages(fixed_jsonl):\n",
        "    pages = {}\n",
        "    with open(fixed_jsonl, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            o = json.loads(line)\n",
        "            pages[o[\"page_index\"]] = o[\"text\"]\n",
        "    return pages\n",
        "\n",
        "def _load_chunks(chunks_jsonl):\n",
        "    chunks = []\n",
        "    with open(chunks_jsonl, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            chunks.append(json.loads(line))\n",
        "    return chunks\n",
        "\n",
        "def coverage_from_chunks_v2(fixed_jsonl, chunks_jsonl, para_min_len=80):\n",
        "    pages  = _load_pages(fixed_jsonl)\n",
        "    chunks = _load_chunks(chunks_jsonl)\n",
        "\n",
        "    # page -> concat des textes de chunks qui déclarent cette page\n",
        "    by_page_concat = {p:\"\" for p in pages}\n",
        "    for c in chunks:\n",
        "        txt = c[\"text\"]\n",
        "        for p in c.get(\"pages\", []):\n",
        "            if p in by_page_concat:\n",
        "                by_page_concat[p] += (\"\\n\\n\" + txt)\n",
        "\n",
        "    report = []\n",
        "    for p, page_text in pages.items():\n",
        "        page_paras = _paragraphs(page_text, min_len=para_min_len)\n",
        "        spans = []\n",
        "        haystack = by_page_concat.get(p, \"\")\n",
        "        for para in page_paras:\n",
        "            if para and para in haystack:\n",
        "                start = page_text.find(para)\n",
        "                if start != -1:\n",
        "                    spans.append((start, start + len(para)))\n",
        "        covered = _union_len(spans)\n",
        "        cov_pct = 0.0 if len(page_text)==0 else (covered/len(page_text))*100.0\n",
        "        report.append({\n",
        "            \"page_index\": p,\n",
        "            \"page_len\": len(page_text),\n",
        "            \"covered_len\": covered,\n",
        "            \"coverage_pct\": round(cov_pct, 3),\n",
        "            \"paras_count\": len(page_paras),\n",
        "            \"paras_matched\": len(spans)\n",
        "        })\n",
        "    return sorted(report, key=lambda x: x[\"page_index\"])\n"
      ],
      "metadata": {
        "id": "PkrceA4V-pde"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fixed  = \"/content/0018_LeveragingSecurity_DK_20160111_fixed.jsonl\"\n",
        "chunks = \"/content/0018_LeveragingSecurity_DK_20160111_chunks.jsonl\"\n",
        "\n",
        "rep = coverage_from_chunks_v2(fixed, chunks, para_min_len=80)\n",
        "for r in rep:\n",
        "    print(r)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "to3i8B4H-xmJ",
        "outputId": "05bcd9af-b287-4127-94ff-334e381e1fa8"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'page_index': 0, 'page_len': 1837, 'covered_len': 1837, 'coverage_pct': 100.0, 'paras_count': 1, 'paras_matched': 1}\n",
            "{'page_index': 1, 'page_len': 3207, 'covered_len': 3207, 'coverage_pct': 100.0, 'paras_count': 1, 'paras_matched': 1}\n",
            "{'page_index': 2, 'page_len': 3050, 'covered_len': 3050, 'coverage_pct': 100.0, 'paras_count': 1, 'paras_matched': 1}\n",
            "{'page_index': 3, 'page_len': 1251, 'covered_len': 1251, 'coverage_pct': 100.0, 'paras_count': 1, 'paras_matched': 1}\n",
            "{'page_index': 4, 'page_len': 2503, 'covered_len': 2503, 'coverage_pct': 100.0, 'paras_count': 1, 'paras_matched': 1}\n",
            "{'page_index': 5, 'page_len': 1917, 'covered_len': 1917, 'coverage_pct': 100.0, 'paras_count': 1, 'paras_matched': 1}\n",
            "{'page_index': 6, 'page_len': 2551, 'covered_len': 2551, 'coverage_pct': 100.0, 'paras_count': 1, 'paras_matched': 1}\n",
            "{'page_index': 7, 'page_len': 1217, 'covered_len': 1217, 'coverage_pct': 100.0, 'paras_count': 1, 'paras_matched': 1}\n",
            "{'page_index': 8, 'page_len': 4128, 'covered_len': 4128, 'coverage_pct': 100.0, 'paras_count': 1, 'paras_matched': 1}\n",
            "{'page_index': 9, 'page_len': 2143, 'covered_len': 2143, 'coverage_pct': 100.0, 'paras_count': 1, 'paras_matched': 1}\n",
            "{'page_index': 10, 'page_len': 1812, 'covered_len': 1812, 'coverage_pct': 100.0, 'paras_count': 1, 'paras_matched': 1}\n",
            "{'page_index': 11, 'page_len': 1035, 'covered_len': 1035, 'coverage_pct': 100.0, 'paras_count': 1, 'paras_matched': 1}\n",
            "{'page_index': 12, 'page_len': 2144, 'covered_len': 2144, 'coverage_pct': 100.0, 'paras_count': 1, 'paras_matched': 1}\n",
            "{'page_index': 13, 'page_len': 1179, 'covered_len': 1179, 'coverage_pct': 100.0, 'paras_count': 1, 'paras_matched': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Étape suivante (RAG texte only) — ingestion VectorDB (FAISS)**"
      ],
      "metadata": {
        "id": "GdVjFt1dFgpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install faiss-cpu sentence-transformers\n",
        "\n",
        "import json, numpy as np, faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 1) modèle multilingue orienté recherche\n",
        "model = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
        "\n",
        "# 2) charger tes chunks\n",
        "chunks_jsonl = \"/content/0018_LeveragingSecurity_DK_20160111_chunks.jsonl\"\n",
        "texts, metas = [], []\n",
        "with open(chunks_jsonl, encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        o = json.loads(line)\n",
        "        txt = o[\"text\"].strip()\n",
        "        if not txt: continue\n",
        "        # IMPORTANT: préfixe passage\n",
        "        texts.append(\"passage: \" + txt)\n",
        "        metas.append({\"id\": i, \"pages\": o.get(\"pages\", [])})\n",
        "\n",
        "# 3) embeddings normalisés + index FAISS (cosine via Inner Product)\n",
        "emb = model.encode(texts, normalize_embeddings=True)  # (N, d)\n",
        "index = faiss.IndexFlatIP(emb.shape[1])\n",
        "index.add(emb.astype(\"float32\"))\n",
        "\n",
        "faiss.write_index(index, \"/content/security.e5.index\")\n",
        "with open(\"/content/security.e5.meta.json\",\"w\",encoding=\"utf-8\") as f:\n",
        "    json.dump({\"metas\": metas}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"OK index E5:\", len(texts))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bccrvgn1FmY5",
        "outputId": "ffcd667a-4128-4164-aba2-44f06563a5a4"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK index E5: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recherche sémantique (top-k)**"
      ],
      "metadata": {
        "id": "nU08i8gnFqxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, faiss, json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "index = faiss.read_index(\"/content/security.e5.index\")\n",
        "with open(\"/content/security.e5.meta.json\", encoding=\"utf-8\") as f:\n",
        "    METAS = json.load(f)[\"metas\"]\n",
        "model = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
        "\n",
        "MINF = -3.4028235e+38  # min float32\n",
        "\n",
        "def search_e5(query: str, k: int = 8, threshold: float = 0.0):\n",
        "    k = min(k, index.ntotal)  # borne dure\n",
        "    q = model.encode([\"query: \" + query], normalize_embeddings=True).astype(\"float32\")\n",
        "    D, I = index.search(q, k)\n",
        "\n",
        "    out = []\n",
        "    for rank, (idx, score) in enumerate(zip(I[0], D[0]), 1):\n",
        "        if idx == -1 or score <= (MINF/2):  # garde que les vrais hits\n",
        "            continue\n",
        "        row = {\"rank\": rank, \"score\": float(score), **METAS[idx]}\n",
        "        if row[\"score\"] >= threshold:\n",
        "            out.append(row)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "LgZULIUiFtn8"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hits = search_e5(\"procédure d’installation sécurisée du composant X\", k=8, threshold=0.5)\n",
        "for h in hits:\n",
        "    print(h)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p28j7xAQNP3-",
        "outputId": "b0571b6e-3f83-4439-ada7-5dafc2d5ea5a"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rank': 1, 'score': 0.7873495817184448, 'id': 0, 'pages': [0, 1, 2, 3]}\n",
            "{'rank': 2, 'score': 0.7849428057670593, 'id': 3, 'pages': [10, 11, 12, 13]}\n",
            "{'rank': 3, 'score': 0.7848808765411377, 'id': 2, 'pages': [7, 8, 9]}\n",
            "{'rank': 4, 'score': 0.7676029205322266, 'id': 1, 'pages': [4, 5, 6]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pretty_hits(hits, max_items=8):\n",
        "    for h in hits[:max_items]:\n",
        "        print(f\"[{h['rank']}] score={h['score']:.3f}  pages={h['pages']}  id={h['id']}\")\n",
        "\n",
        "def filter_hits(hits, threshold=0.6, k=4):\n",
        "    kept = [h for h in hits if h[\"score\"] >= threshold]\n",
        "    return kept[:k]\n",
        "\n",
        "pretty_hits(hits)\n",
        "top_hits = filter_hits(hits, threshold=0.6, k=3)\n",
        "print(\"\\nKept:\", len(top_hits))\n",
        "pretty_hits(top_hits)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsAWRmW9OBC9",
        "outputId": "af20b5bb-76eb-444f-8e90-7d9fad793db1"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] score=0.787  pages=[0, 1, 2, 3]  id=0\n",
            "[2] score=0.785  pages=[10, 11, 12, 13]  id=3\n",
            "[3] score=0.785  pages=[7, 8, 9]  id=2\n",
            "[4] score=0.768  pages=[4, 5, 6]  id=1\n",
            "\n",
            "Kept: 3\n",
            "[1] score=0.787  pages=[0, 1, 2, 3]  id=0\n",
            "[2] score=0.785  pages=[10, 11, 12, 13]  id=3\n",
            "[3] score=0.785  pages=[7, 8, 9]  id=2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, re\n",
        "\n",
        "chunks_jsonl = \"/content/0018_LeveragingSecurity_DK_20160111_chunks.jsonl\"  # adapte si besoin\n",
        "\n",
        "def get_chunk_text(chunk_id: int, path=chunks_jsonl) -> str:\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i == chunk_id:\n",
        "                return json.loads(line)[\"text\"]\n",
        "    return \"\"\n",
        "\n",
        "_SENT_SPLIT = re.compile(r'(?<=[.!?])\\s+')\n",
        "\n",
        "def extractive_answer(query: str, hits: list, max_chars=1200, top_k=3):\n",
        "    kept = hits[:top_k]\n",
        "    snippets = []\n",
        "    used = 0\n",
        "    for h in kept:\n",
        "        txt = get_chunk_text(h[\"id\"]).strip()\n",
        "        if not txt:\n",
        "            continue\n",
        "        # sélection naïve: phrases qui contiennent des mots de la requête\n",
        "        q_terms = {w.lower() for w in re.findall(r\"\\w+\", query) if len(w) >= 3}\n",
        "        sentences = _SENT_SPLIT.split(txt)\n",
        "        scored = []\n",
        "        for s in sentences:\n",
        "            terms = {w.lower() for w in re.findall(r\"\\w+\", s)}\n",
        "            overlap = len(q_terms & terms)\n",
        "            if overlap:\n",
        "                scored.append((overlap, s))\n",
        "        scored.sort(key=lambda x: x[0], reverse=True)\n",
        "        picked = \" \".join([s for _, s in scored[:5]]) or \" \".join(sentences[:3])\n",
        "\n",
        "        room = max_chars - used\n",
        "        if room <= 0:\n",
        "            break\n",
        "        piece = picked[:room]\n",
        "        snippets.append(f\"- {piece}\")\n",
        "        used += len(piece)\n",
        "\n",
        "    answer = \"\\n\".join(snippets) if snippets else \"(pas de phrase très spécifique trouvée; voir extraits ci-dessous)\"\n",
        "    sources = [f\"pages {h['pages']} (score {h['score']:.3f}, id {h['id']})\" for h in kept]\n",
        "    return answer, sources\n",
        "\n",
        "# exemple avec ta requête\n",
        "q = \"procédure d’installation sécurisée du composant X\"\n",
        "answer, sources = extractive_answer(q, top_hits, max_chars=1200, top_k=3)\n",
        "\n",
        "print(\"=== RÉPONSE (extraction) ===\\n\", answer[:1200])\n",
        "print(\"\\n=== SOURCES ===\")\n",
        "for s in sources: print(\"•\", s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_WRSD-zOEn6",
        "outputId": "d376f703-3c44-4b62-d0c0-024df087f3b0"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== RÉPONSE (extraction) ===\n",
            " - WHITE PAPER Leveraging Security - Using the SEL RTAC's Built-In Security Features Darrin Kite INTRODUCTION Cyberthreats to critical infrastructure represent a growing and persistent risk to a nation's security and prosperity. The SEL Real-Time Automation Controller (RTAC) product platform serves as the information hub for substations in electric utilities as well as many other critical industries. It provides essential services, such as data aggregation, logic processing, oscillography, event report collection, and secure engineering access.\n",
            "- 11 SECTION 7: SECURITY AUDITING - EVENT MONITORING AND REPORTING Tracking and archiving events is an important function provided by a data concentrator. In the event that an actual or suspected unauthorized intrusion is detected, a device's logs provide an essential piece of forensic evidence. In the SEL RTAC, the SOE log can store up to 30,000 log items.\n",
            "- 8 Figure 7 An ACSELERATOR RTAC Project Configured for Secure Engineering Access to Three IEDs Communicating Via SEL Protocol The SEL server also supports the capability to enforce authentication before granting access to the command line interface. Note that when SSH is the serial tunnel\n",
            "\n",
            "=== SOURCES ===\n",
            "• pages [0, 1, 2, 3] (score 0.787, id 0)\n",
            "• pages [10, 11, 12, 13] (score 0.785, id 3)\n",
            "• pages [7, 8, 9] (score 0.785, id 2)\n"
          ]
        }
      ]
    }
  ]
}